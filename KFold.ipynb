{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611111111111111"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9796296296296296"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by dividing the data by using train_test_split it is not the best way because,\n",
    "# if training set contain all the questions related to algebra and test set contain question related,\n",
    "# to calculus then our model will not perform well, \n",
    "# so by using cross validation we can avoid this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in kfold cross validation we split the total data into folds eg take 5\n",
    "# in 1st model take svm 1st fold is test set and remaninig 2, 3, 4, 5 folds are training set\n",
    "# then we train the model(svm1) on training set and test the testset and will note the score of model1\n",
    "# in 2nd model 2nd fold is test set and remaining 1, 3, 4, 5 flods are training set,\n",
    "# with this data we train the 2nd model(svm2) and will note the score.\n",
    "# we do this for 5 times and create 5 svm models, svm1, svm2,..,svm5,\n",
    "# now we average the scores of all 5 svms,\n",
    "# now we will note the average score for svm\n",
    "# like this we calculate the scores for all models (DecisionT, LogisticRegression, RandomForest, ....)\n",
    "# finally we take the best model with good average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by using cross validation we can choose the best model eg(svm, or logisticregression or .....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:purple'>KFold cross validation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use KFold for our digits example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    183\n",
       "1    182\n",
       "5    182\n",
       "4    181\n",
       "6    181\n",
       "9    180\n",
       "7    179\n",
       "0    178\n",
       "2    177\n",
       "8    174\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts() # from 0-9 classification total 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# n_splits is no of folds\n",
    "kf = KFold(n_splits=3)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8] [0 1 2]\n",
      "[0 1 2 6 7 8] [3 4 5]\n",
      "[0 1 2 3 4 5] [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# kf.split([1,2,3,4,5,6,7,8,9]) will split into 3 folds one fold for testing and 2 folds for training\n",
    "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
    "    print(train_index, test_index)\n",
    "# in 1st fold test index is 0, 1, 2 remaining indexs are train indexes\n",
    "# in 2nd fold test index is 3, 4, 5 remaining indexs are train indexes\n",
    "# in 3rd fold test index is 6, 7, 8 remaining indexs are train indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "# for kf no need y(target) because it will go in sequence of test data\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], \\\n",
    "                                       y[train_index], y[test_index]\n",
    "    \n",
    "    scores_logistic.append(get_score(LogisticRegression(solver='liblinear', multi_class='ovr'),\n",
    "                                     X_train, X_test, y_train, y_test))  \n",
    "    scores_svm.append(get_score(SVC(gamma='auto'), \n",
    "                                X_train, X_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40),\n",
    "                               X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8964941569282137, 0.9515859766277128, 0.9115191986644408]\n",
      "[0.41068447412353926, 0.41569282136894825, 0.4273789649415693]\n",
      "[0.9365609348914858, 0.9482470784641068, 0.9115191986644408]\n"
     ]
    }
   ],
   "source": [
    "print(scores_logistic)\n",
    "print(scores_svm)\n",
    "print(scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9198664440734557\n",
      "0.41791875347801893\n",
      "0.9321090706733445\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array(scores_logistic)))\n",
    "print(np.mean(np.array(scores_svm)))\n",
    "print(np.mean(np.array(scores_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold is similar to KFold it is better when seperating folds based on target categories,\n",
    "# it will create folds on each classification(target) category in an uniform way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# we are creating three arrays for three models\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "# for dividing folds uniformly based on target variable we need to supply target variable also(y)\n",
    "# but for kfold no need to supply target variable(y) just X is enough\n",
    "# but in skf both X and y are needed\n",
    "for train_index, test_index in folds.split(X, y):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], \\\n",
    "                                       y[train_index], y[test_index]\n",
    "    \n",
    "    # for every training and testing folds classifiers get traind \n",
    "    # as we did 3folds 3 scores are created for each classifier\n",
    "    scores_logistic.append(get_score(LogisticRegression(solver='liblinear', multi_class='ovr'),\n",
    "                                     X_train, X_test, y_train, y_test))  \n",
    "    scores_svm.append(get_score(SVC(gamma='auto'), \n",
    "                                X_train, X_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40),\n",
    "                               X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9193099610461881\n",
      "0.4346132442960489\n",
      "0.9354479688369505\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array(scores_logistic)))\n",
    "print(np.mean(np.array(scores_svm)))\n",
    "print(np.mean(np.array(scores_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8948247078464107, 0.9532554257095158, 0.9098497495826378]\n",
      "[0.3806343906510851, 0.41068447412353926, 0.5125208681135225]\n",
      "[0.9298831385642737, 0.9532554257095158, 0.9232053422370617]\n"
     ]
    }
   ],
   "source": [
    "print(scores_logistic)\n",
    "print(scores_svm)\n",
    "print(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:purple'>cross_val_score function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression model performance using cross_val_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8948247078464107, 0.9532554257095158, 0.9098497495826378]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.89482471, 0.95325543, 0.90984975])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here cv is no of folds\n",
    "# if cv=3 means there are 3 folds which means 3 test datas which means 3 models which means 3 scores\n",
    "# the below code will do the same thing as stratifiedkfold for loop\n",
    "print(scores_logistic)\n",
    "cross_val_score(LogisticRegression(solver='liblinear', multi_class='ovr'), X, y, cv=3)\n",
    "# from below outputs we can observe that both outputs are nearly same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**svm model performance using cross_val_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3806343906510851, 0.41068447412353926, 0.5125208681135225]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.38063439, 0.41068447, 0.51252087])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores_svm)\n",
    "cross_val_score(SVC(gamma='auto'), X, y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**random forest performance using cross_val_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9298831385642737, 0.9532554257095158, 0.9232053422370617]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92153589, 0.94490818, 0.92153589])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores_rf)\n",
    "cross_val_score(RandomForestClassifier(n_estimators=40), X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score uses stratifiedkfold as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:purple'>Parameter tunning using k fold cross validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also do parameter tuning with cross_val_score\n",
    "# instead of using different classifiers we use the same classifier with different parameters,\n",
    "# which is parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87222222, 0.91111111, 0.89444444, 0.77777778, 0.88333333,\n",
       "       0.9       , 0.92222222, 0.93854749, 0.79888268, 0.8547486 ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=5), X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8686871508379888"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=5), X, y, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365704531346989"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=20), X, y, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382371198013656"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=30), X, y, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942675356921167"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=40), X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used cross_val_score to\n",
    "fine tune our random forest classifier and figured that having around 40 trees in random forest gives best result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
